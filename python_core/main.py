import os
import sys
from flask import Flask, request, Response
from flask_cors import CORS
from dotenv import load_dotenv
import litellm
import json
import logging

# --- Initialization ---
load_dotenv()

app = Flask(__name__)
CORS(app)

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- LiteLLM Configuration ---
# Uncomment the line below to enable debugging
# litellm.set_verbose=True

# --- Smart Router Logic ---
def check_api_keys():
    """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„APIå¯†é’¥"""
    openai_key = os.environ.get('OPENAI_API_KEY')
    deepseek_key = os.environ.get('DEEPSEEK_API_KEY')
    anthropic_key = os.environ.get('ANTHROPIC_API_KEY')
    ollama_url = os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434')
    
    available_models = []
    warnings = []
    
    if openai_key and openai_key != 'your_openai_api_key_here':
        available_models.extend(['gpt-4o', 'gpt-3.5-turbo'])
        logging.info("âœ… OpenAI APIå¯†é’¥å·²é…ç½®")
    else:
        warnings.append("âš ï¸  OpenAI APIå¯†é’¥æœªé…ç½® - é«˜å¤æ‚åº¦ä»»åŠ¡å’Œè§†è§‰åˆ†æå°†ä¸å¯ç”¨")
    
    if deepseek_key and deepseek_key != 'your_deepseek_api_key_here':
        available_models.append('deepseek/deepseek-chat')
        logging.info("âœ… DeepSeek APIå¯†é’¥å·²é…ç½®")
    else:
        warnings.append("âš ï¸  DeepSeek APIå¯†é’¥æœªé…ç½® - ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡å°†ä¸å¯ç”¨")
    
    if anthropic_key and anthropic_key != 'your_anthropic_api_key_here':
        available_models.extend(['claude-3-haiku', 'claude-3-sonnet'])
        logging.info("âœ… Anthropic APIå¯†é’¥å·²é…ç½®")
    else:
        warnings.append("âš ï¸  Anthropic APIå¯†é’¥æœªé…ç½® - Claudeæ¨¡å‹å°†ä¸å¯ç”¨")
    
    # æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
    try:
        import requests
        response = requests.get(f"{ollama_url}/api/tags", timeout=2)
        if response.status_code == 200:
            available_models.append('ollama/llama3')
            logging.info("âœ… Ollamaæœ¬åœ°æœåŠ¡å™¨å¯ç”¨")
        else:
            warnings.append("âš ï¸  Ollamaæœ¬åœ°æœåŠ¡å™¨ä¸å¯ç”¨ - æœ¬åœ°å…è´¹æ¨¡å‹å°†ä¸å¯ç”¨")
    except:
        warnings.append("âš ï¸  Ollamaæœ¬åœ°æœåŠ¡å™¨ä¸å¯ç”¨ - æœ¬åœ°å…è´¹æ¨¡å‹å°†ä¸å¯ç”¨")
    
    if warnings:
        for warning in warnings:
            logging.warning(warning)
    
    if not available_models:
        logging.error("âŒ æ²¡æœ‰å¯ç”¨çš„AIæ¨¡å‹ï¼è¯·é…ç½®è‡³å°‘ä¸€ä¸ªAPIå¯†é’¥ã€‚")
        logging.info("ğŸ“– è¯·æŸ¥çœ‹ python_core/config_example.env äº†è§£å¦‚ä½•é…ç½®")
    
    return available_models, warnings

def route_model(content, has_image=False):
    """æ ¹æ®å†…å®¹å¤æ‚åº¦å’Œç±»å‹é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹"""
    available_models, _ = check_api_keys()
    
    if not available_models:
        return None, "æ²¡æœ‰å¯ç”¨çš„AIæ¨¡å‹ï¼Œè¯·é…ç½®APIå¯†é’¥"
    
    # å¦‚æœæœ‰å›¾åƒï¼Œä¼˜å…ˆä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
    if has_image:
        if 'gpt-4o' in available_models:
            return 'gpt-4o', None
        else:
            return None, "å›¾åƒåˆ†æéœ€è¦OpenAI APIå¯†é’¥"
    
    # æ–‡æœ¬ä»»åŠ¡æ ¹æ®å¤æ‚åº¦é€‰æ‹©
    text_length = len(str(content))
    
    if text_length < 100:  # ç®€å•ä»»åŠ¡ - ä¼˜å…ˆä½¿ç”¨ä¾¿å®œæˆ–å…è´¹çš„æ¨¡å‹
        if 'ollama/llama3' in available_models:
            return 'ollama/llama3', None
        elif 'claude-3-haiku' in available_models:
            return 'claude-3-haiku', None
        elif 'deepseek/deepseek-chat' in available_models:
            return 'deepseek/deepseek-chat', None
        elif 'gpt-3.5-turbo' in available_models:
            return 'gpt-3.5-turbo', None
    elif text_length < 500:  # ä¸­ç­‰ä»»åŠ¡ - å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
        if 'deepseek/deepseek-chat' in available_models:
            return 'deepseek/deepseek-chat', None
        elif 'claude-3-sonnet' in available_models:
            return 'claude-3-sonnet', None
        elif 'gpt-3.5-turbo' in available_models:
            return 'gpt-3.5-turbo', None
        elif 'ollama/llama3' in available_models:
            return 'ollama/llama3', None
    else:  # å¤æ‚ä»»åŠ¡ - ä½¿ç”¨æœ€å¼ºæ¨¡å‹
        if 'gpt-4o' in available_models:
            return 'gpt-4o', None
        elif 'claude-3-sonnet' in available_models:
            return 'claude-3-sonnet', None
        elif 'deepseek/deepseek-chat' in available_models:
            return 'deepseek/deepseek-chat', None
        elif 'gpt-3.5-turbo' in available_models:
            return 'gpt-3.5-turbo', None
    
    # fallbackåˆ°ä»»ä½•å¯ç”¨æ¨¡å‹
    if available_models:
        return available_models[0], None
    
    return None, "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"

# --- API Endpoint ---
@app.route('/chat/completions', methods=['POST'])
def chat_completions():
    """èŠå¤©å®Œæˆç«¯ç‚¹ - å…¼å®¹OpenAI APIæ ¼å¼"""
    try:
        data = request.json
        if not data:
            return Response(json.dumps({"error": "æ²¡æœ‰æä¾›è¯·æ±‚æ•°æ®"}), 
                          status=400, mimetype='application/json; charset=utf-8')
        
        messages = data.get('messages', [])
        if not messages:
            return Response(json.dumps({"error": "æ²¡æœ‰æä¾›æ¶ˆæ¯"}), 
                          status=400, mimetype='application/json; charset=utf-8')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒ
        has_image = any(
            isinstance(msg.get('content'), list) and 
            any(item.get('type') == 'image_url' for item in msg['content'])
            for msg in messages
        )
        
        # é€‰æ‹©æ¨¡å‹
        model, error = route_model(messages, has_image)
        if error:
            return Response(json.dumps({"error": error}), 
                          status=503, mimetype='application/json; charset=utf-8')
        
        logging.info(f"ğŸ§  ä½¿ç”¨æ¨¡å‹: {model} (å›¾åƒ: {has_image})")
        
        # è°ƒç”¨LiteLLM
        try:
            response = litellm.completion(
                model=model,
                messages=messages,
                stream=data.get('stream', False),
                max_tokens=data.get('max_tokens', 4096),
                temperature=data.get('temperature', 0.7),
            )
            
            if data.get('stream', False):
                # æµå¼å“åº”
                def generate():
                    for chunk in response:
                        yield f"data: {json.dumps(chunk.dict())}\n\n"
                    yield "data: [DONE]\n\n"
                
                return Response(generate(), mimetype='text/plain')
            else:
                # éæµå¼å“åº”
                return Response(json.dumps(response.dict(), ensure_ascii=False), 
                              mimetype='application/json; charset=utf-8')
                
        except Exception as api_error:
            logging.error(f"APIè°ƒç”¨å¤±è´¥: {api_error}")
            return Response(json.dumps({"error": f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(api_error)}"}), 
                          status=500, mimetype='application/json; charset=utf-8')
            
    except Exception as e:
        logging.error(f"è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return Response(json.dumps({"error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 
                      status=500, mimetype='application/json; charset=utf-8')

# --- Health Check Endpoint ---
@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    available_models, warnings = check_api_keys()
    
    status = {
        "status": "healthy" if available_models else "warning", 
        "message": "LiteLLM Agent æ­£åœ¨è¿è¡Œ",
        "available_models": available_models,
        "warnings": warnings
    }
    
    return Response(json.dumps(status, ensure_ascii=False), 
                   mimetype='application/json; charset=utf-8')

# --- Main Execution ---
if __name__ == '__main__':
    try:
        port = int(os.environ.get('FLASK_PORT', os.environ.get('PORT', 5001)))
        host = os.environ.get('FLASK_HOST', '127.0.0.1')
        
        logging.info("ğŸ”¥ LiteLLM æ™ºèƒ½è·¯ç”±ä»£ç†æ­£åœ¨å¯åŠ¨...")
        
        # æ£€æŸ¥é…ç½®
        available_models, warnings = check_api_keys()
        
        if not available_models:
            logging.warning("ğŸš¨ è­¦å‘Šï¼šæ²¡æœ‰å¯ç”¨çš„AIæ¨¡å‹ï¼")
            logging.warning("ğŸ“– è¯·æŸ¥çœ‹ python_core/config_example.env äº†è§£å¦‚ä½•é…ç½®APIå¯†é’¥")
            logging.warning("ğŸ”§ æœåŠ¡å™¨ä»ä¼šå¯åŠ¨ï¼Œä½†AIåŠŸèƒ½å°†ä¸å¯ç”¨")
        
        logging.info(f"ğŸŒ æœåŠ¡å™¨å¯åŠ¨äº http://{host}:{port}")
        logging.info("ğŸ“Š FlaskæœåŠ¡å™¨è¿è¡Œä¸­...")
        
        app.run(host=host, port=port, debug=False)
        
    except KeyboardInterrupt:
        logging.info("ğŸ›‘ æœåŠ¡å™¨è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logging.error(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1) 